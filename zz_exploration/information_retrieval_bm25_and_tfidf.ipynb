{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d2aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from src.connection import client\n",
    "from src.config import index_name\n",
    "from src.utils import process_query, construct_elasticsearch_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1138e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "model = 'en_core_sci_sm'\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "# List of stop words to be added\n",
    "stop_words = ['.', ':', ',', '(',')', '[',']','?', '\\\\','/', '+', '-','\\\"','\\'','1','2',' ']\n",
    "# Add stop words to nlp.vocab\n",
    "for word in stop_words:\n",
    "    nlp.vocab[word].is_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73038da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = {\"query\": {\"match_all\": {}}}\n",
    "response = client.search(index=index_name, body=query, scroll=\"2m\", size=1000)  # Adjust size based on your needs\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "docs=[]\n",
    "while True:\n",
    "    # Process the current batch of results\n",
    "    for hit in response['hits']['hits']:\n",
    "        docs.append(hit[\"_source\"])\n",
    "        \n",
    "    response = client.scroll(scroll_id=response['_scroll_id'], scroll='2m')\n",
    "    if not response['hits']['hits']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bcb175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# step = len(docs)/10\n",
    "# start_t = datetime.now()\n",
    "# for i, doc in enumerate(docs):\n",
    "#     text = doc[\"Title\"] + \" \" + doc[\"Abstract\"]\n",
    "#     tokens = [token.text.lower() for token in nlp(text)]\n",
    "#     tokenized_text = \" \".join([token for token in tokens if not nlp.vocab[token].is_stop])\n",
    "#     corpus.append(tokenized_text)\n",
    "\n",
    "#     if (i+1) % step  == 0:\n",
    "#         with open(\"data/preprocessed_corpus.pkl\", \"wb\") as f:\n",
    "#             pickle.dump(corpus, f)\n",
    "            \n",
    "#         end_t = datetime.now() \n",
    "#         print(f\"Preprocessing progress: {(i+1) * 100 / len(docs):.1f}%. Spend {(end_t-start_t).total_seconds()/60:.2f} minutes until now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fed1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save preprocessed_corpus to a pickle file\n",
    "# with open(\"data/preprocessed_corpus.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7cb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed_corpus from the pickle file\n",
    "with open(\"data/preprocessed_corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125e7a8",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87466a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key cognitive abilities associated human intelligence\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key cognitive abilities associated with human intelligence?\"\n",
    "tokens, entities = process_query(query,nlp)\n",
    "tokenized_query = \" \".join(tokens)\n",
    "print(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BM25 scores\n",
    "k1=1.6\n",
    "b= 0.5\n",
    "bm25 = BM25Okapi(corpus,k1=k1, b=b)\n",
    "scores = bm25.get_scores(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3edf6580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Score 145.52693536266966\n",
      "35476562 Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification.\n",
      "==================================================\n",
      "Rank 2: Score 145.38190541553934\n",
      "33838025 [Teleradiology-based stroke network in Western and Southern Transdanubia in Hungary].\n",
      "==================================================\n",
      "Rank 3: Score 145.3439609080656\n",
      "33270387 [From psychoanalysis to psychodynamic psychotherapy at Albert-Prevost].\n",
      "==================================================\n",
      "Rank 4: Score 145.33504813365943\n",
      "33584450 Developing an Instrument for Assessing Self-Efficacy in Data Mining and Analysis.\n",
      "==================================================\n",
      "Rank 5: Score 145.3080217841081\n",
      "33486897 Generalized neurocognitive impairment in individuals at ultra-high risk for psychosis: The possible key role of slowed processing speed.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "bm25_ranked_docs_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "\n",
    "# Specify the date range for PubDateEDAT facet search\n",
    "pub_date_range = [\"2019/01/01\", \"2023/12/31\"]\n",
    "start_date = datetime.strptime(pub_date_range[0], \"%Y/%m/%d\")\n",
    "end_date = datetime.strptime(pub_date_range[1], \"%Y/%m/%d\")\n",
    "\n",
    "# Filter documents based on PubDateEDAT facet\n",
    "bm25_filtered_docs = [(scores[i],docs[i]) for i in bm25_ranked_docs_indices if start_date <= datetime.strptime(docs[i][\"PubDateEDAT\"], \"%Y/%m/%d\") <= end_date]\n",
    "\n",
    "# Display top N similar documents\n",
    "top_n = min(5,len(bm25_filtered_docs))\n",
    "for i, (score, doc) in enumerate(bm25_filtered_docs[:top_n]):\n",
    "    print(f\"Rank {i + 1}: Score {score}\")\n",
    "    print(doc[\"PMID\"],doc[\"Title\"])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03582f0",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1375fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key cognitive abilities associated human intelligence\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key cognitive abilities associated with human intelligence?\"\n",
    "tokens, entities = process_query(query,nlp)\n",
    "tokenized_query = \" \".join(tokens)\n",
    "print(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6fefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "tfidf_corpus = copy.deepcopy(corpus)\n",
    "tfidf_corpus.append(tokenized_query)\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l2')\n",
    "tfidf_embeddings = vectorizer.fit_transform(tfidf_corpus)\n",
    "\n",
    "tfidf_scores = cosine_similarity(tfidf_embeddings[:-1], tfidf_embeddings[-1].reshape(1, -1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20c91fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Score 145.52693536266966\n",
      "35476562 Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification.\n",
      "==================================================\n",
      "Rank 2: Score 145.38190541553934\n",
      "33838025 [Teleradiology-based stroke network in Western and Southern Transdanubia in Hungary].\n",
      "==================================================\n",
      "Rank 3: Score 145.3439609080656\n",
      "33270387 [From psychoanalysis to psychodynamic psychotherapy at Albert-Prevost].\n",
      "==================================================\n",
      "Rank 4: Score 145.33504813365943\n",
      "33584450 Developing an Instrument for Assessing Self-Efficacy in Data Mining and Analysis.\n",
      "==================================================\n",
      "Rank 5: Score 145.3080217841081\n",
      "33486897 Generalized neurocognitive impairment in individuals at ultra-high risk for psychosis: The possible key role of slowed processing speed.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "tfidf_ranked_docs_indices = sorted(range(len(tfidf_scores)), key=lambda i: scores[i], reverse=True)\n",
    "\n",
    "# Specify the date range for PubDateEDAT facet search\n",
    "pub_date_range = [\"2019/01/01\", \"2023/12/31\"]\n",
    "start_date = datetime.strptime(pub_date_range[0], \"%Y/%m/%d\")\n",
    "end_date = datetime.strptime(pub_date_range[1], \"%Y/%m/%d\")\n",
    "\n",
    "# Filter documents based on PubDateEDAT facet\n",
    "tfidf_filtered_docs = [(scores[i],docs[i]) for i in tfidf_ranked_docs_indices if start_date <= datetime.strptime(docs[i][\"PubDateEDAT\"], \"%Y/%m/%d\") <= end_date]\n",
    "\n",
    "# Display top N similar documents\n",
    "top_n = min(5,len(tfidf_filtered_docs))\n",
    "for i, (score, doc) in enumerate(tfidf_filtered_docs[:top_n]):\n",
    "    print(f\"Rank {i + 1}: Score {score}\")\n",
    "    print(doc[\"PMID\"],doc[\"Title\"])\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
