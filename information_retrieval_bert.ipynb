{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d2aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from src.connection import client\n",
    "from src.config import index_name\n",
    "from src.utils import process_query, construct_elasticsearch_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1138e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "model = 'en_core_sci_sm'\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "# List of stop words to be added\n",
    "stop_words = ['.', ':', ',', '(',')', '[',']','?', '\\\\','/', '+', '-','\\\"','\\'','1','2',' ']\n",
    "# Add stop words to nlp.vocab\n",
    "for word in stop_words:\n",
    "    nlp.vocab[word].is_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73038da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = {\"query\": {\"match_all\": {}}}\n",
    "response = client.search(index=index_name, body=query, scroll=\"2m\", size=1000)  # Adjust size based on your needs\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "docs=[]\n",
    "while True:\n",
    "    # Process the current batch of results\n",
    "    for hit in response['hits']['hits']:\n",
    "        docs.append(hit[\"_source\"])\n",
    "        \n",
    "    response = client.scroll(scroll_id=response['_scroll_id'], scroll='2m')\n",
    "    if not response['hits']['hits']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7072285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# step = len(docs)/10\n",
    "# start_t = datetime.now()\n",
    "# for i, doc in enumerate(docs):\n",
    "#     text = doc[\"Title\"] + \" \" + doc[\"Abstract\"]\n",
    "#     tokens = [token.text.lower() for token in nlp(text)]\n",
    "#     tokenized_text = \" \".join([token for token in tokens if not nlp.vocab[token].is_stop])\n",
    "#     corpus.append(tokenized_text)\n",
    "\n",
    "#     if (i+1) % step  == 0:\n",
    "#         with open(\"data/preprocessed_corpus.pkl\", \"wb\") as f:\n",
    "#             pickle.dump(corpus, f)\n",
    "            \n",
    "#         end_t = datetime.now() \n",
    "#         print(f\"Preprocessing progress: {(i+1) * 100 / len(docs):.1f}%. Spend {(end_t-start_t).total_seconds()/60:.2f} minutes until now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fed1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save preprocessed_corpus to a pickle file\n",
    "# with open(\"data/preprocessed_corpus.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7cb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed_corpus from the pickle file\n",
    "with open(\"data/preprocessed_corpus.pkl\", \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae7043",
   "metadata": {},
   "source": [
    "## SentenceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70310043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key cognitive abilities associated human intelligence\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key cognitive abilities associated with human intelligence?\"\n",
    "tokens, entities = process_query(query,nlp)\n",
    "tokenized_query = \" \".join(tokens)\n",
    "print(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eda94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Similarly, you can batch process the query if needed\n",
    "query_embedding = model.encode(tokenized_query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b153a0",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dce7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents in batches\n",
    "# batch_size = 64\n",
    "# corpus_batches = [corpus[i:i + batch_size] for i in range(0, len(corpus), batch_size)]\n",
    "\n",
    "# doc_embeddings_list = []\n",
    "# start_t = datetime.now()\n",
    "# for i, batch in enumerate(corpus_batches):\n",
    "#     doc_embeddings_batch = model.encode(batch, convert_to_tensor=True)\n",
    "#     doc_embeddings_list.append(doc_embeddings_batch)\n",
    "    \n",
    "#     if (i+1) % 100  == 0:\n",
    "#         end_t = datetime.now()\n",
    "#         print(f\"progress: {(i+1) * 100 / len(corpus_batches):.1f}%. Spend {(end_t-start_t).total_seconds()/60:.2f} minutes until now\")\n",
    "        \n",
    "# doc_embeddings = torch.cat(doc_embeddings_list)\n",
    "# np.save('data/doc_embeddings_v2.npy', doc_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a4168",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed60a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained SentenceTransformer model\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Encode documents and query\n",
    "# doc_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# np.save('data/doc_embeddings_v1.npy', doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbcbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data/doc_embeddings.npy', doc_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16eb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings back\n",
    "doc_embeddings = np.load('data/doc_embeddings_v1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f94be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between query and documents\n",
    "st_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f90d86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Score 0.5498866438865662\n",
      "27150661 Executive function and intelligence in the resolution of temporary syntactic ambiguity: an individual differences investigation.\n",
      "==================================================\n",
      "Rank 2: Score 0.546873927116394\n",
      "27750571 Cognitive Deficits Post-Traumatic Brain Injury and Their Association with Injury Severity and Gray Matter Volumes.\n",
      "==================================================\n",
      "Rank 3: Score 0.5415911674499512\n",
      "27809665 Executive abilities in children with congenital visual impairment in mid-childhood.\n",
      "==================================================\n",
      "Rank 4: Score 0.5327783226966858\n",
      "27726852 Does the way we read others' mind change over the lifespan? Insights from a massive web poll of cognitive skills from childhood to late adulthood.\n",
      "==================================================\n",
      "Rank 5: Score 0.5259026885032654\n",
      "27825737 Profile of cognitive function in adults with duchenne muscular dystrophy.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "ranked_docs_indices = sorted(range(len(st_scores)), key=lambda i: st_scores[i], reverse=True)\n",
    "\n",
    "# Specify the date range for PubDateEDAT facet search\n",
    "start_date = datetime.strptime(\"2016/01/01\", \"%Y/%m/%d\")\n",
    "end_date = datetime.strptime(\"2016/12/31\", \"%Y/%m/%d\")\n",
    "\n",
    "# Filter documents based on PubDateEDAT facet\n",
    "filtered_docs = [(st_scores[i],docs[i]) for i in ranked_docs_indices if start_date <= datetime.strptime(docs[i][\"PubDateEDAT\"], \"%Y/%m/%d\") <= end_date]\n",
    "\n",
    "# Display top N similar documents\n",
    "top_n = min(5,len(filtered_docs))\n",
    "for i, (score, doc) in enumerate(filtered_docs[:top_n]):\n",
    "    print(f\"Rank {i + 1}: Score {score}\")\n",
    "    print(doc[\"PMID\"],doc[\"Title\"])\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
