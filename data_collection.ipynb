{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba20264d",
   "metadata": {},
   "source": [
    "#### 1. install EDirect\n",
    "sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n",
    "\n",
    "#### 2. fetch all PubMed ids \n",
    "esearch -db pubmed -query \"intelligence[tiab] AND 2013:2023[dp] AND hasabstract\" | efetch -format uid > pmids.csv\n",
    "\n",
    "#### 3. fetch abstract of all the articles based on PMID as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5581b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez, Medline\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "\n",
    "Entrez.email = \"huqiaowen0104@gmail.com\"  # Set your email address\n",
    "\n",
    "pmids_file_path = 'data/pmids.csv'\n",
    "\n",
    "index_name = \"pubmed_intelligence\"\n",
    "\n",
    "host = '127.0.0.1'\n",
    "port = 9200\n",
    "username = 'admin'\n",
    "password = 'admin'\n",
    "\n",
    "client = OpenSearch(hosts = [{'host': host, 'port': port}],\n",
    "                        http_auth =(username, password),\n",
    "                        use_ssl = True,\n",
    "                        verify_certs = False,\n",
    "                        ssl_assert_hostname = False,\n",
    "                        ssl_show_warn = False,\n",
    "                        timeout=30\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db594a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pubmed_record(pmid):\n",
    "    try:\n",
    "        # Fetch the record from PubMed\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"medline\", retmode=\"text\")\n",
    "        # print(handle.read())\n",
    "        records = Medline.parse(handle)\n",
    "        record = next(records)\n",
    "\n",
    "        # Extract information\n",
    "        pubmed_id = record.get(\"PMID\", \"\")\n",
    "        title = record.get(\"TI\", \"\")\n",
    "        abstract = record.get(\"AB\", \"\")\n",
    "        keywords = record.get(\"OT\", [])\n",
    "        authors = record.get(\"AU\", [])\n",
    "        pub_date_edat = record.get(\"EDAT\", \"\")\n",
    "        article_date = record.get(\"CRDT\", \"\")\n",
    "        journal = record.get(\"JT\", \"\")\n",
    "\n",
    "        return {\n",
    "            'PMID': pubmed_id,\n",
    "            'Title': title,\n",
    "            'Abstract': abstract,\n",
    "            'Keywords': keywords,\n",
    "            'Authors': authors,\n",
    "            'PubDateEDAT': pub_date_edat.split(\" \")[0],\n",
    "            'ArticleDate':article_date[0].split(\" \")[0],\n",
    "            'Journal': journal\n",
    "            \n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50f3d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index:\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'pubmed_intelligence'}\n"
     ]
    }
   ],
   "source": [
    "# create index\n",
    "with open(\"pubmed_intelligence_mappings.json\", \"r\") as mapping_file:\n",
    "    mapping_json = mapping_file.read()\n",
    "\n",
    "try:\n",
    "    response = client.indices.create(index_name,body=mapping_json)\n",
    "    print(\"Creating index:\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e14f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "pmids_df = pd.read_csv(pmids_file_path,header=None,names=['pmid'])\n",
    "print('number or docs:',len(pmids_df))\n",
    "\n",
    "batch_size = 100\n",
    "for i in range(0,len(pmids_df),batch_size):\n",
    "    data_batch = []\n",
    "    pmids = pmids_df['pmid'][i:i+batch_size].tolist()\n",
    "    start_t = datetime.now()\n",
    "    for pmid in pmids:\n",
    "        record_data = fetch_pubmed_record(pmid)\n",
    "        record_index = {\"index\": {\"_index\": index_name, \"_id\": record_data[\"PMID\"]}}\n",
    "        data_batch.append(record_index)\n",
    "        data_batch.append(record_data)\n",
    "    \n",
    "    try:\n",
    "        # Bulk index the data\n",
    "        client.bulk(body=data_batch, index=index_name)\n",
    "        end_t = datetime.now()\n",
    "        print(f\"Records starting from {i}+ are stored into opensearch. Cost {(end_t-start_t).total_seconds()} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3656800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58730\n",
      "{'PMID': '26665339', 'Title': 'Using perioperative analytics to optimize OR performance.', 'Abstract': 'In the past, the data hospitals gleaned from operating rooms (ORs) tended to be static and lacking in actionable information. Hospitals can improve OR performance by applying OR analytics, such as evaluation of turnover times and expenses, which provide useful intelligence. Having the information is important, but success depends on aligning staff behavior to effectively achieve improvement strategies identified using the analytics.', 'Keywords': [], 'Authors': ['Rempfer D'], 'PubDateEDAT': '2015/12/17', 'ArticleDate': '2015/12/16', 'Journal': 'Healthcare financial management : journal of the Healthcare Financial Management Association'}\n",
      "Records starting from 0+ are stored into opensearch.\n",
      "Records starting from 10000+ are stored into opensearch.\n",
      "Records starting from 20000+ are stored into opensearch.\n",
      "Records starting from 30000+ are stored into opensearch.\n",
      "Records starting from 40000+ are stored into opensearch.\n",
      "Records starting from 50000+ are stored into opensearch.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Replace 'path_to_your_file.json' with the actual file path\n",
    "file_path = 'data/pubmed_intelligence.json'\n",
    "\n",
    "# Open the file and load its contents\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "batch_size = 10000\n",
    "for i in range(0,len(data),batch_size):\n",
    "    data_batch = []\n",
    "    for record_data in data[i:i+batch_size]:\n",
    "        record_index = {\"index\": {\"_index\": index_name, \"_id\": record_data[\"PMID\"]}}\n",
    "        data_batch.append(record_index)\n",
    "        data_batch.append(record_data)\n",
    "\n",
    "    try:\n",
    "        # Bulk index the data\n",
    "        client.bulk(body=data_batch, index=index_name)\n",
    "        print(f\"Records starting from {i}+ are stored into opensearch.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
