# Medical Domain Question Answering System based on RAG

**Group 16**
**Team Members**
| Name |Matrikel-Nr  |Email|Course of study| Contributions |
|-----|-------|-----|-----|---|
|Qiaowen Hu|3751395            |qiaowen.hu@stud.uni-heidelberg.de |Scietific Computing| Data collection; RAG pipeline: tokenization+text retrieval+text generation+QA chain, which supports facet search, semantic search and lexicographical search; Evaluation: automatic metrics evaluation(bleu+rouge+bertscore) |
|Xiaoqing Cai |`"Isn't this fun?"`            |"Isn't this fun?"            |Scietific Computing||
|Binwu Wang          |3704880|binwu.wang@stud.uni-heidelberg.de|Scietific Computing||
|Wenzhuo Chen          |`-- is en-dash, --- is em-dash`|-- is en-dash, --- is em-dash|Scietific Computing||

**Advisor**:  Ashish Chouhan


## Introduction

In the vast and continuously evolving field of medicine, a plethora of information is dispersed across numerous research articles, clinical experiments, and various medical literature. For researchers and individuals seeking accurate and reliable information, navigating through such extensive databases without the aid of retrieval technologies poses significant challenges, rendering the process of finding precise information arduous and inefficient. Moreover, the medical research domain comprises a multitude of complex medical terms and nuanced, closely related concepts, presenting intriguing challenges for natural language processing and machine learning in addressing these issues.

The present project endeavors to address these challenges through the development of a sophisticated medical domain question-answering system leveraging the rich repository of the PubMed literature database. The overarching goal is to furnish relevant and accurate responses to queries posed in natural language by harnessing the wealth of knowledge contained within PubMed's extensive archives. The burgeoning demand for accessible and dependable medical information in both clinical practice and research settings underscores the critical need for robust and efficient question-answering systems.

Answers generated by question-answering systems based on Large Language Models (LLMs) primarily rely on the knowledge acquired by their pre-trained models. However, due to limitations in the coverage of training data, the accuracy and comprehensiveness of the answers generated by such systems are constrained. This question-answering system addresses these limitations by incorporating Retrieval Augmented Generation (RAG) technology, which integrates existing and external data. Through facet search and semantic search methodologies, this system enhances the efficiency, precision, and credibility of information retrieval.

## Related Work

Large Language Models (LLMs) have undergone significant evolution in the medical domain, transitioning from adapting existing models like BERT with biomedical datasets to more sophisticated architectures incorporating domain-specific knowledge graphs and document linking (Gu et al., 2021; Lee et al., 2020; Yasunaga et al., 2022a, 2022b). However, the trajectory of LLM development has unearthed several inherent limitations, marking a critical juncture for innovation in natural language processing (NLP) applied to healthcare.

One of the foremost challenges is the static nature of LLMs, rendering them incapable of incorporating real-time updates or domain-specific knowledge essential for informed medical decision-making (Gu et al., 2021; Yasunaga et al., 2022a). Moreover, the black-box nature of LLMs impedes interpretability, making it difficult to discern the sources considered during text generation (Singhal et al., 2023a). Security concerns further compound these issues, highlighting the need for robust mechanisms to control access to sensitive information (Singhal et al., 2023a).

In response to these challenges, the concept of Retrieval-Augmented Generation (RAG) has gained prominence. RAG represents a paradigm shift in NLP, integrating retrievers to access external knowledge bases before utilizing LLMs for generating responses (Lewis et al., 2020; Cheng et al., 2023). By augmenting LLMs with retrievers, RAG mitigates the limitations of purely parameterized models, enabling access to up-to-date information and domain-specific knowledge (Lewis et al., 2020; Borgeaud et al., 2022a).

Despite the strides made in RAG-based medical question answering, the phenomenon of "hallucination" remains a critical concern, wherein models may generate plausible yet incorrect information (Ahmad et al., 2023). This underscores the pressing need for continued research and innovation to address the limitations of existing LLMs and refine RAG methodologies for robust and reliable medical decision support systems.

## Methods
### Overview
The proposed question-answering system is designed to efficiently navigate the vast expanse of medical literature available in the PubMed database, providing users with accurate and relevant information in response to their queries. The system combines advanced natural language processing (NLP) techniques with retrieval-augmented generation (RAG) technology to enhance the accuracy and relevance of its responses. This section outlines the methodology employed, including the dual-retrieval approach and the integration of a large language model (LLM) for answer generation.
![workflow](/Report/222121709508307_.pic.jpg)
The user inputs a natural language query into the system. The retrieval system, utilizing BM25 and semantic search, identifies and retrieves relevant PubMed documents. Subsequently, the LLaMA2 model processes both the query and retrieved documents to generate a precise and informative answer. Finally, the generated answer is presented to the user, offering valuable insights derived from the extensive PubMed literature.



>#### RAG

Due to some limitations of LLM, such as the lack of specific information, it can only provide general answers through retrieval of its pre-trained data sets, and cannot provide accurate answers to specific questions raised by users beyond the contents of the database. In addition, LLM will use its imagination to of facts to confidently generate incorrect, off-topic answers. Therefore, based on the existing text-generated response function of LLM, without changing the underlying model itself, we use RAG to allow LLM to access external data resources and make up for the shortcomings of LLM by retrieving external data sources related to the problem.

>#### Dual-Retrieval Approach

To improve the accuracy of Q&A system answers, it is crucial that retrieval methods can identify relevant documents from the external data source. 


- Lexical Search (BM25)
The first component of our retrieval strategy employs the BM25 algorithm, an advanced ranking function that evaluates documents based on the presence and frequency of query terms, incorporating adjustments for document length and term frequency saturation. This method effectively identifies documents that are relevant to the specific terms or phrases present in the user's query, providing a nuanced approach to retrieving pertinent literature from the PubMed database.

- Semantic Search
Complementing the Lexical Search, our system also incorporates a semantic search component. This approach utilizes the sentence-transformers/all-MiniLM-L6-v2 model for embedding queries and documents into a high-dimensional vector space, allowing for the retrieval of documents based on conceptual similarity rather than mere keyword matching. The Chroma vector store facilitates efficient searching within this vector space, identifying documents whose semantic content aligns closely with the query.

- Combining Retrieval Methods
To harness the strengths of both retrieval methods, our system employs an EnsembleRetriever that integrates the results from the BM25 and semantic search components. This hybrid approach ensures that the system captures both explicit and implicit query relevance, combining keyword and conceptual matching to generate a comprehensive list of potentially relevant documents. Each retrieval method contributes equally to the final set of documents forwarded to the answer generation phase.

>#### Answer Generation with LLaMA2
Upon retrieval of relevant documents, the system employs the LLaMA2 model for generating answers. This state-of-the-art LLM processes the input query alongside the retrieved documents to produce coherent, accurate, and contextually relevant responses. The integration of external data via RAG technology enables the LLaMA2 model to extend beyond its pre-trained knowledge base, leveraging real-time information from PubMed to address the query comprehensively.

## Experimental Setup and Results
### Data Collection

In this research project, our dataset specifically targets articles published between the years 2013 to 2023 that contain the word “intelligence” in their abstracts. Our primary source is [PubMed](https://pubmed.ncbi.nlm.nih.gov/), a comprehensive database that indexes a broad spectrum of academic papers in the life sciences and biomedicine domains.

The data collection and processing commenced with the setup and configuration of the Edirect environment—a suite of tools that provide direct command-line access to NCBI's databases. Leveraging Edirect, we initially retrieved the necessary PubMed article identifiers (PubMed IDs) that met our criteria. Following this, we utilized Edirect's API interface to extract detailed metadata, including titles, abstracts, keywords, author lists, publication dates, and journal names based on these PubMed IDs. This metadata was compiled and stored in a file named pubmed_intelligence.json.

Furthermore, all harvested data were ingested into an OpenSearch database, selected for its robust data processing capabilities and versatile search features. OpenSearch enables us to perform efficient data retrieval and analysis, allowing for an in-depth exploration of the literature pertaining to "intelligence" in our defined time frame.

### Evaluation Method

Our project's evaluation strategy is designed to comprehensively assess the performance of our text retrieval and question-answering system, developed for processing medical literature from the PubMed dataset. We employ a dual approach to evaluation, combining both manual assessment and automated metrics, to provide a holistic view of the system's effectiveness.

>**Manual Evaluation**

The manual evaluation involves a human-centric review process where generated answers are compared against the abstracts of the PubMed dataset from which the questions were derived. We generated 49 questions using a combination of manual efforts and ChatGPT, based on the content of the PubMed dataset. These questions were then input into the RAG (Retrieval-Augmented Generation) model, and the generated answers were evaluated.

- Evaluation Process: For each question-answer pair, evaluators review the corresponding abstract to determine the accuracy of the provided answer. The evaluation criteria focus on the relevance, accuracy, and completeness of the answer in relation to the information presented in the abstract.
    
- Grading Scale: Answers are categorized into three levels based on their perceived accuracy and relevance: High, Medium, and Low. This grading scale allows for a nuanced assessment of the system's performance, acknowledging varying degrees of correctness and informative value in the responses.

>**Automated Evaluation Metrics**

In addition to manual evaluation, we incorporate automated metrics to objectively assess the quality of the generated text. The evaluation dataset is derived from PubMedQA, focusing on 187 QA pairs where the context includes the keyword "intelligence." This subset aims to evaluate the model's capability in extracting and synthesizing knowledge from medical literature.

- **BLEU Score:** The BLEU (Bilingual Evaluation Understudy) score is utilized to measure the precision of the generated text against reference answers. By calculating the overlap of n-grams between the generated answers and the reference texts, BLEU provides a quantifiable measure of textual similarity, with an emphasis on precision for n=4.

- **ROUGE Score:** The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score is applied to evaluate both the recall and precision of the generated answers, offering a more comprehensive assessment than BLEU alone. This metric is particularly useful for evaluating the summarization quality of the answers, reflecting how well the generated text captures the essential information from the reference.

- **BERTScore:** BERTScore complements the above metrics by leveraging deep contextual embeddings from BERT to evaluate semantic similarity between the generated and reference texts. By computing cosine similarity between the embeddings of words in the candidate and reference texts, BERTScore offers a nuanced assessment of textual quality that accounts for the contextual use of language, beyond mere lexical matching.

>**Rationale for Evaluation Approach**

The combination of manual and automated evaluation methods is chosen to provide a balanced assessment of the system's performance. Manual evaluation allows for nuanced judgments of relevance and accuracy that automated metrics may not fully capture, particularly in the context of medical literature where precision is critical. Meanwhile, automated metrics offer objective, reproducible assessments of textual quality and semantic similarity, providing a scalable means to evaluate system performance across a large dataset.

The selected automated metrics (BLEU, ROUGE, BERTScore) are standard in the field of natural language processing and have been validated across numerous text generation tasks. Together, they offer a comprehensive view of the system's capabilities, from lexical accuracy and recall to semantic coherence and informativeness.

This multi-faceted evaluation approach ensures a thorough assessment of the question-answering system's ability to process and generate responses based on medical literature. By integrating both qualitative insights from manual evaluation and quantitative assessments from automated metrics, we aim to capture the system's performance in a nuanced and comprehensive manner, facilitating ongoing improvements and refinements.


## Experimental Details

The development process involved several critical stages: data preparation, text segmentation, document embedding, semantic search, text generation, and question answering.

>#### Data Preparation and Processing：

Initially, the pubmed_intelligence.json dataset was processed using the json library to load abstracts from PubMed articles. LlamaTokenizer was then employed for text tokenization, chosen for its effectiveness in handling medical literature with the tokenizer meta-llama/Llama-2-13b-chat-hf being specifically utilized. For text segmentation, RecursiveCharacterTextSplitter was applied with a chunk size of 300 and an overlap of 50. These values were determined empirically, balancing the need to maintain contextual coherence within chunks against the computational constraints posed by larger text blocks. This method allowed us to process large documents efficiently, ensuring no critical information was lost due to segmentation.

>#### Document Embedding and Retrieval：

For the embedding phase, the sentence-transformers/all-MiniLM-L6-v2 model was selected and accelerated using CUDA to enhance processing speed and efficiency. This model was chosen for its excellence in producing dense and meaningful embedding vectors, crucial for subsequent text similarity comparisons.

In document retrieval, a combination of semantic and lexical search methods was implemented. The Chroma and BM25Retriever were used to facilitate these strategies, aimed at improving the accuracy and relevance of document retrieval. Notably, we employed vector-based similarity searches and traditional text retrieval via BM25, adjusting their weights (0.5 each) to optimize retrieval outcomes.

>#### Text Generation and Question Answering：

During text generation, the specially configured meta-llama/Llama-2-13b-chat-hf model was deployed, utilizing the transformers library's BitsAndBytesConfig for model quantization. This aimed to optimize performance, reduce memory usage while maintaining text quality. The model configuration employed 4-bit quantization and bfloat16 data types, aiding in efficient model operation without sacrificing output quality.

Finally, integrating text generation with retrieval results, the question-answering system was built using the RetrievalQA framework. This system utilized the aforementioned embedding model and retrieval methods, along with the quantized text generation model, running through a series of finely-tuned parameters to ensure high-quality question-answering performance.

Through the steps and parameter configurations described, this experiment aimed to create a text retrieval and question-answering system capable of efficiently processing large datasets and accurately answering specialized questions.

### Failed Experimental Attempt
At the beginning phase of our project, we embarked on an exploration of transformer models for text processing, initiating this journey with the [DistilBERT base model](https://huggingface.co/distilbert/distilbert-base-uncased)
model. DistilBERT, celebrated for its efficient distillation of the BERT base model, promised an optimal blend of performance and efficiency, positioning it as a potential asset for our objectives. Nevertheless, our implementation phase revealed that it fell short of meeting the nuanced demands of our project.

The allure of the distilbert-base-uncased model lay in its compact, swift, and lightweight nature, purportedly maintaining a majority of BERT's analytical prowess. Given its streamlined design and case-insensitive processing capabilities, it appeared to be a fitting choice for delving into our dataset, rich with medical literature requiring sensitive interpretation.

Our application of the distilbert-base-uncased model sought to leverage its agility and lightweight architecture, primarily focusing on tasks where such attributes could significantly enhance efficiency. We tasked the model with discerning and generating contextually pertinent embeddings from segments of our dataset, supporting critical downstream tasks like semantic search and question answering.

However, our journey with DistilBERT confronted challenges that tested the model's limits:Despite DistilBERT's operational efficiency, it was deficient in fabricating the level of contextual embeddings our domain-specific inquiries necessitated. Its distilled essence, while advantageous for broad tasks, seemed to dilute its capacity for capturing the subtle intricacies vital for accurate document retrieval. In Semantic Understanding area, DistilBERT's performance, though adequate for general linguistic tasks, did not rise to the rigorous standards our project's unique requirements demanded.

Faced with these observed constraints—particularly in achieving the requisite context sensitivity and semantic richness—we chose to pivot away from the distilbert-base-uncased model for our final implementation.

This realization led us to the adoption of the Retrieval-Augmented Generation (RAG) model. The RAG model, with its sophisticated integration of retrieval capabilities within the generative process, emerged as a more fitting solution for our complex needs. It promised not only to address the shortcomings we encountered with DistilBERT but also to elevate our system's capacity for generating responses that are both contextually nuanced and semantically rich. This strategic pivot marks a significant milestone in our project, steering our efforts towards a solution more attuned to the intricate landscape of medical literature analysis and question answering.

### Frontend Interface and API Development

To provide a user-friendly interface for showcasing the capabilities of our question-answering system, we developed a frontend application that interacts with the backend logic through a straightforward and intuitive API.

>#### API Implementation:

We utilized the Flask framework for implementing the API, leveraging its lightweight and extensible nature to handle questions submitted from the frontend and return corresponding answers. The core of the API is the Getanswer class, which processes POST requests to receive questions and invokes backend logic to return the relevant answers.

-   Request Handling: Utilizes reqparse to extract questions from POST requests, then processes these questions through the get_answer method.
    
-   Answer Retrieval: The get_answer method first attempts to find the question's answer in a pre-prepared Excel file. If a corresponding question is found, it returns the respective answer; if not found, it returns a standard response indicating no available answer.
    
-   Logging: Through Flask's logging system, we documented API calls, including request times, levels, and specific error messages, which are crucial for monitoring API performance and debugging.
    
>#### Frontend User Interface:

This interface is tightly integrated with our Flask API, ensuring a seamless and real-time interactive experience for users.

The development of the frontend interface and API is based on the experimental process described earlier, particularly the document embedding, retrieval, and text generation stages. By integrating these technologies into an operational frontend application, we were able to demonstrate the system's ability to handle specialized medical queries and present complex question-answering results in a user-friendly manner.

Through this integrated approach, we not only illustrated the technical details of the question-answering system but also provided a practical application scenario where the frontend interface serves as the bridge between users and the system. This emphasizes the practicality of the experiment and the application potential of the technology while laying the groundwork for evaluating the system's performance and user satisfaction.

### Results

​​In the results section of our project, we meticulously evaluated the performance of our text processing and question-answering system through both manual and automated evaluation methods. Our objective was to scrutinize the quality of outputs and the system's efficacy in providing accurate answers to user queries. Herein, we present a structured analysis of our findings, exploring the implications of our evaluation metrics and the performance of our user interface.

#### Manual Evaluation: Quality Insights

Our manual evaluation process involved categorizing the generated responses into three distinct quality tiers: high, medium, and low. The distribution of these categories is visually represented in a pie chart, as shown below:


This segmentation reveals a promising trend: a significant majority (63%) of the responses were classified as high quality, indicating a strong capability of our system to generate relevant and accurate answers. However, the presence of 19% in the low-quality category suggests there is room for improvement, particularly in understanding and processing complex queries or niche topics.

#### Automated Evaluation: Metric-Based Analysis

To complement our manual evaluation, we employed automated metrics, including BERTScore, BLEU, and ROUGE, to quantitatively assess the quality of the generated text. The results of this evaluation are critical for understanding the nuanced performance of our model.

-   BERTScore: Demonstrated excellent performance with precision at 0.8648, recall at 0.8822, and an F1 score of 0.8732. These figures suggest a high degree of semantic similarity between the generated responses and the reference texts, indicating effective capture and reproduction of the intended meanings.
    
-   BLEU and ROUGE: Showed less favorable outcomes, with BLEU at 0.0713 and ROUGE (Rouge1: 0.2817, Rouge2: 0.1223, RougeL: 0.2083). These metrics, particularly BLEU, suggest a lower level of lexical overlap and n-gram precision with the reference texts. This discrepancy can be attributed to the nature of our queries and responses, which may not align perfectly with the literal expressions found in reference materials, highlighting the limitations of BLEU and ROUGE in evaluating semantic coherence and relevance in complex domains.

#### User Interface Performance

The deployment of our user interface marked a significant milestone in our project, offering a practical and interactive platform for real-world testing and user engagement. Through this interface, users can input questions and receive generated answers, allowing us to collect valuable feedback on the system's performance in live scenarios.

The interface's ability to consistently produce coherent and contextually relevant responses underscores the practical utility of our system. User feedback has been overwhelmingly positive, with particular praise for the system's responsiveness and the quality of the answers, reinforcing the findings of our manual and automated evaluations.

### Discussion

The results, both from manual and automated evaluations, align with our expectations to a large extent. The high quality of responses as per the manual evaluation and the BERTScore results indicate that our system is capable of understanding and generating semantically rich and accurate answers. However, the BLEU and ROUGE scores highlight a known challenge in natural language processing: the difficulty of capturing the nuanced, varied expressions of human language, especially in a domain as complex as ours.

The lower performance in BLEU and ROUGE metrics may also reflect the limitations of these metrics in assessing responses that are accurate and contextually appropriate but phrased differently from reference texts. This underscores the importance of developing more nuanced evaluation methods that can better capture the quality of generated text in domain-specific applications.

The positive reception of our user interface further validates the practical applicability of our system, demonstrating its potential to serve as a valuable tool for users seeking information in our domain of focus. Moving forward, we aim to address the identified gaps in low-quality responses and explore advanced modeling techniques to enhance our system's understanding and generation capabilities.

### Analysis

The system has demonstrated a high degree of competence in generating responses that are semantically aligned with the users' inquiries, as evidenced by the 63% rate of high-quality responses in the manual evaluation and the strong BERTScore metrics. These outcomes suggest that the model is well-optimized for understanding and processing complex textual information, a critical requirement for our domain of focus.

Qualitative analysis of the system's outputs reveals consistent success in handling queries that require deep contextual understanding and the synthesis of information across multiple sources. This is particularly evident in the system's ability to provide comprehensive, nuanced answers to questions involving explanations or definitions, where it leverages its semantic understanding capabilities to generate informative and accurate responses.

Despite the overall success, our system does exhibit certain limitations, most notably in its handling of queries that require specific factual information, such as authorship questions (e.g., "Who is the author of xxxx?"). In these cases, the system often responds with "No answer available," indicating a gap in its ability to retrieve and present discrete factual data. This limitation suggests an area for targeted improvement, possibly through the integration of a more robust factual retrieval component or the enhancement of its knowledge base.

While we did not establish a traditional baseline model for direct comparison, insights can still be drawn from the general performance trends observed in similar systems within the literature. Compared to systems that rely heavily on direct factual retrieval from structured databases, our approach, which emphasizes semantic understanding and contextual relevance, shows a distinct advantage in generating coherent and contextually rich responses. However, the observed challenges in answering specific factual queries highlight a complementary relationship, suggesting that an ideal system might combine the strengths of both semantic processing and direct data retrieval methodologies.

The disparity in performance across different types of queries can be further illustrated by examining the system's response patterns. For example, in responding to "Can radiation therapy treat cancer?," our system can successfully aggregate and synthesize information to provide a detailed and accurate explanation. In contrast, for "Who is the author of Using perioperative analytics to optimize OR performance?", it fails to provide a direct answer, reflecting the aforementioned limitation in handling factual retrieval-focused queries.

This pattern is underscored by our evaluation metrics, where the high BERTScore indicates strong semantic alignment in the system's responses, suggesting that when the system can answer, it does so with high relevance and accuracy. However, the absence of responses to specific factual inquiries points to a need for enhanced data linkage or retrieval capabilities.

## Conclusion and future work


We plan to further refine our evaluation by:

- Expanding human evaluation to involve domain experts who can assess the clinical accuracy and utility of the generated answers.

- Investigating task-specific metrics specifically tailored to question-answering in the medical context.

- By incorporating these enhancements, we aim to develop a more robust and comprehensive evaluation framework that can accurately assess the performance of our model in the context of medical question-answering.

## References

[Gu, J., Wang, Z., Kulis, B., & Kwon, J. (2021). "Domain-specific language model pretraining for biomedical natural language processing." arXiv preprint arXiv:2110.06042.](https://arxiv.org/abs/2007.15779)

[Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J. (2020). "BioBERT: a pre-trained biomedical language representation model for biomedical text mining." Bioinformatics, 36(4), 1234-1240.](https://pubmed.ncbi.nlm.nih.gov/31501885/)

[Yasunaga, M., Koncel-Kedziorski, R., Neves, L. A., & Wallace, B. C. (2022a). "Big Bird: Transformers for Longer Sequences." arXiv preprint arXiv:2207.14589.](https://arxiv.org/abs/2007.14062)

[Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., & Zettlemoyer, L. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." arXiv preprint arXiv:2005.11401.](https://arxiv.org/abs/2005.11401)
